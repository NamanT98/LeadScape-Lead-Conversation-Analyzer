{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/user/DATADRIVE1/project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "logging.basicConfig(filename='training_logs_model_T5.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.info(\"Training started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)  # Use float for binary classification\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv(\"chat_summary_labels.csv\")  # Load your CSV file with data\n",
    "test_texts = data['T5'][:150]  \n",
    "test_labels = data['labels'][:150]\n",
    "train_texts = data['T5'][150:]  \n",
    "train_labels = data['labels'][150:]\n",
    "# Tokenizer and Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = ConversationDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_length=128)\n",
    "test_dataset = ConversationDataset(test_texts.tolist(), test_labels.tolist(), tokenizer, max_length=128)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Customer is looking for a laptop that can hand...\n",
      "1      Customers of their digestive health supplement...\n",
      "2      Sellerman is interested in investing in the st...\n",
      "3      Salesman, managing director of Financial Plann...\n",
      "4      Customer is looking for a smartphone with a go...\n",
      "                             ...                        \n",
      "145    Salesman is consulting customer about buying a...\n",
      "146    Salesman, a company that provides a range of h...\n",
      "147    Salesman, who has been in the tech industry fo...\n",
      "148    Sellerman is interested in buying a new laptop...\n",
      "149    Salesman, who has been researching other tech ...\n",
      "Name: T5, Length: 150, dtype: object 0      0.0\n",
      "1      1.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "      ... \n",
      "145    1.0\n",
      "146    0.0\n",
      "147    1.0\n",
      "148    0.0\n",
      "149    0.0\n",
      "Name: labels, Length: 150, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class BertWithDropoutAndBatchNorm(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(BertWithDropoutAndBatchNorm, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "\n",
    "        # Batch Normalization and Dropout layers\n",
    "        self.batch_norm = nn.BatchNorm1d(768)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        bert_outputs = self.bert.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = bert_outputs.last_hidden_state  # Shape: (batch_size, seq_len, 768)\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)\n",
    "        normalized_output = self.batch_norm(pooled_output)\n",
    "        dropped_out_output = self.dropout(normalized_output)\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.bert.classifier(dropped_out_output)  # Use the existing classification layer\n",
    "        probs = self.sigmoid(logits)  # Apply sigmoid for binary classification\n",
    "        return probs\n",
    "model = BertWithDropoutAndBatchNorm(dropout_rate=0.3).to(device)\n",
    "\n",
    "\n",
    "# class BertForBinaryClassification(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BertForBinaryClassification, self).__init__()\n",
    "#         self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)  # Binary classification, single output\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits\n",
    "#         probs = self.sigmoid(logits)  # Apply Sigmoid to the logits\n",
    "#         return probs\n",
    "\n",
    "# # Initialize model\n",
    "# model = BertForBinaryClassification()\n",
    "# model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/user/DATADRIVE1/project/env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_training_steps = len(train_loader) * 20  # 20 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for batch in tqdm(dataloader,\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            # 'labels': batch['label'].to(device)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.squeeze(), batch['label'].to(device))  # Squeeze to match label shape\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_form_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = (outputs.squeeze() > 0.5).float()  # Get predictions as binary 0 or 1\n",
    "        correct_preds += (preds == batch['label'].to(device)).sum().item()\n",
    "        total_preds += len(batch['label'].to(device))\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = correct_preds / total_preds\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "from tqdm import tqdm\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader,\"Validation\"):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.squeeze()  # Squeeze for binary classification\n",
    "            val_loss = criterion(outputs.squeeze(), batch['label'].to(device)) \n",
    "            preds = (logits > 0.5).cpu().numpy()  # Convert logits to binary (0 or 1)\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct_preds += (preds == batch['label'].cpu().numpy()).sum()\n",
    "            total_preds += len(batch['label'])\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return predictions, true_labels, val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  8.00it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.7159, Train Accuracy = 0.5275, Val Loss = 0.7565791606903076, Val Acc = 0.44666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  8.00it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.6648, Train Accuracy = 0.5900, Val Loss = 0.8136405944824219, Val Acc = 0.4866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.99it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.6212, Train Accuracy = 0.6725, Val Loss = 0.8064589500427246, Val Acc = 0.47333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.96it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.5633, Train Accuracy = 0.7225, Val Loss = 1.104245662689209, Val Acc = 0.4866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.96it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.5213, Train Accuracy = 0.7625, Val Loss = 0.896377444267273, Val Acc = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.93it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.4855, Train Accuracy = 0.7875, Val Loss = 1.1165916919708252, Val Acc = 0.4866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.92it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.4369, Train Accuracy = 0.8250, Val Loss = 1.1096055507659912, Val Acc = 0.49333333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.92it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.3670, Train Accuracy = 0.8375, Val Loss = 1.1533582210540771, Val Acc = 0.5333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.91it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.2995, Train Accuracy = 0.8825, Val Loss = 1.590253233909607, Val Acc = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.89it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.2573, Train Accuracy = 0.9075, Val Loss = 1.5497071743011475, Val Acc = 0.5066666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.89it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 27.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.2139, Train Accuracy = 0.9525, Val Loss = 1.7394745349884033, Val Acc = 0.4533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.88it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 27.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.2158, Train Accuracy = 0.9375, Val Loss = 1.323854923248291, Val Acc = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.89it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.1479, Train Accuracy = 0.9550, Val Loss = 1.6653919219970703, Val Acc = 0.4866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.89it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 27.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.1639, Train Accuracy = 0.9400, Val Loss = 2.0375962257385254, Val Acc = 0.4666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:06<00:00,  7.88it/s]\n",
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 27.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.1284, Train Accuracy = 0.9600, Val Loss = 2.17820405960083, Val Acc = 0.4866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    test_preds, test_labels, test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}, Val Loss = {test_loss}, Val Acc = {test_acc}\")\n",
    "    logging.info(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}, Val Loss = {test_loss}, Val Acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 19/19 [00:00<00:00, 28.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_labels, test_loss, test_acc = evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation\n",
    "# logging.info(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "# logging.info(\"Classification Report:\")\n",
    "# logging.info(classification_report(test_labels, test_preds))\n",
    "# logging.info(\"ROC-AUC Score: %.4f\" % roc_auc_score(test_labels, test_preds))\n",
    "\n",
    "# # Save Model\n",
    "# model.save_pretrained(\"bert_intent_classifier\")\n",
    "# tokenizer.save_pretrained(\"bert_intent_classifier\")\n",
    "\n",
    "# logging.info(\"Model saved successfully.\")\n",
    "\n",
    "# # Testing New Inputs\n",
    "# def predict(model, tokenizer, texts, device):\n",
    "#     model.eval()\n",
    "#     encodings = tokenizer(texts, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "#     input_ids = encodings['input_ids'].to(device)\n",
    "#     attention_mask = encodings['attention_mask'].to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#         probs = outputs.squeeze()  # Apply sigmoid during evaluation\n",
    "#     return probs.cpu().numpy()  # Return raw probability of class 1 (satisfied)\n",
    "\n",
    "# new_texts = [\"Customer is happy with the support provided.\"]\n",
    "# probabilities = predict(model, tokenizer, new_texts, device)\n",
    "# logging.info(\"Satisfaction Probabilities: %s\" % str(probabilities))\n",
    "\n",
    "# print(\"Training complete. Check 'training_logs_model_T5.txt' for detailed logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import numpy as np\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Setting up logging\n",
    "logging.basicConfig(filename='training_logs.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.info(\"Training started.\")\n",
    "# Dataset Class\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)  # Use float for binary classification\n",
    "        }\n",
    "# Load Data\n",
    "data = pd.read_csv(\"chat_summary_labels.csv\")  # Load your CSV file with data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['T5'], data['labels'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer and Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = ConversationDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_length=128)\n",
    "test_dataset = ConversationDataset(test_texts.tolist(), test_labels.tolist(), tokenizer, max_length=128)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "# Model Class\n",
    "class BertForBinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertForBinaryClassification, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)  # Binary classification, single output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = self.sigmoid(logits)  # Apply Sigmoid to the logits\n",
    "        return probs\n",
    "\n",
    "# Initialize model\n",
    "model = BertForBinaryClassification()\n",
    "model=model.to(device)\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_loader) * 20  # 20 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "criterion = nn.BCELoss()\n",
    "# Training Function\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for batch in tqdm(dataloader,\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            # 'labels': batch['label'].to(device)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.squeeze(), batch['label'].to(device))  # Squeeze to match label shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = (outputs.squeeze() > 0.5).float()  # Get predictions as binary 0 or 1\n",
    "        correct_preds += (preds == batch['label'].to(device)).sum().item()\n",
    "        total_preds += len(batch['label'].to(device))\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = correct_preds / total_preds\n",
    "    return epoch_loss, epoch_acc\n",
    "# Evaluation Function\n",
    "from tqdm import tqdm\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader,\"Validation\"):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.squeeze()  # Squeeze for binary classification\n",
    "            val_loss = criterion(outputs.squeeze(), batch['label'].to(device)) \n",
    "            preds = (logits > 0.5).cpu().numpy()  # Convert logits to binary (0 or 1)\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct_preds += (preds == batch['label'].cpu().numpy()).sum()\n",
    "            total_preds += len(batch['label'])\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return predictions, true_labels, val_loss, accuracy\n",
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    test_preds, test_labels, test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}, Val Loss = {test_loss}, Val Acc = {test_acc}\")\n",
    "    logging.info(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}, Val Loss = {test_loss}, Val Acc = {test_acc}\")\n",
    "test_preds, test_labels, test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from transformers import AdamW, get_scheduler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "# import torch.nn as nn\n",
    "# import logging\n",
    "# import numpy as np\n",
    "# # Check for GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# # Setting up logging\n",
    "# logging.basicConfig(filename='training_logs.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "# logging.info(\"Training started.\")\n",
    "# # Dataset Class\n",
    "# class ConversationDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_length):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         encoding = self.tokenizer(\n",
    "#             text, \n",
    "#             max_length=self.max_length, \n",
    "#             padding='max_length', \n",
    "#             truncation=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         return {\n",
    "#             'input_ids': encoding['input_ids'].squeeze(0),\n",
    "#             'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "#             'label': torch.tensor(label, dtype=torch.float)  # Use float for binary classification\n",
    "#         }\n",
    "# # Load Data\n",
    "# data = pd.read_csv(\"chat_summary_labels.csv\")  # Load your CSV file with data\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "#     data['T5'], data['labels'], test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Tokenizer and Dataset\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# train_dataset = ConversationDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_length=128)\n",
    "# test_dataset = ConversationDataset(test_texts.tolist(), test_labels.tolist(), tokenizer, max_length=128)\n",
    "\n",
    "# # DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "# # Model Class\n",
    "# class BertForBinaryClassification(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BertForBinaryClassification, self).__init__()\n",
    "#         self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)  # Binary classification, single output\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits\n",
    "#         probs = self.sigmoid(logits)  # Apply Sigmoid to the logits\n",
    "#         return probs\n",
    "\n",
    "# # Initialize model\n",
    "# model = BertForBinaryClassification()\n",
    "# model=model.to(device)\n",
    "# # Optimizer and Scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# num_training_steps = len(train_loader) * 20  # 20 epochs\n",
    "# lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# # Binary Cross-Entropy Loss\n",
    "# criterion = nn.BCELoss()\n",
    "# # Training Function\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct_preds = 0\n",
    "#     total_preds = 0\n",
    "#     for batch in tqdm(dataloader,\"Training\"):\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = {\n",
    "#             'input_ids': batch['input_ids'].to(device),\n",
    "#             'attention_mask': batch['attention_mask'].to(device),\n",
    "#             # 'labels': batch['label'].to(device)\n",
    "#         }\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = criterion(outputs.squeeze(), batch['label'].to(device))  # Squeeze to match label shape\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         preds = (outputs.squeeze() > 0.5).float()  # Get predictions as binary 0 or 1\n",
    "#         correct_preds += (preds == batch['label'].to(device)).sum().item()\n",
    "#         total_preds += len(batch['label'].to(device))\n",
    "\n",
    "#     epoch_loss = total_loss / len(dataloader)\n",
    "#     epoch_acc = correct_preds / total_preds\n",
    "#     return epoch_loss, epoch_acc\n",
    "# # Evaluation Function\n",
    "# from tqdm import tqdm\n",
    "# def evaluate(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     predictions, true_labels = [], []\n",
    "#     correct_preds = 0\n",
    "#     total_preds = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader,\"Validation\"):\n",
    "#             inputs = {\n",
    "#                 'input_ids': batch['input_ids'].to(device),\n",
    "#                 'attention_mask': batch['attention_mask'].to(device)\n",
    "#             }\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.squeeze()  # Squeeze for binary classification\n",
    "#             val_loss = criterion(outputs.squeeze(), batch['label'].to(device)) \n",
    "#             preds = (logits > 0.5).cpu().numpy()  # Convert logits to binary (0 or 1)\n",
    "#             predictions.extend(preds)\n",
    "#             true_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "#             # Calculate accuracy\n",
    "#             correct_preds += (preds == batch['label'].cpu().numpy()).sum()\n",
    "#             total_preds += len(batch['label'])\n",
    "\n",
    "#     accuracy = correct_preds / total_preds\n",
    "#     return predictions, true_labels, val_loss, accuracy\n",
    "# # Training Loop\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
    "#     test_preds, test_labels, test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "#     print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}, Val Loss = {test_loss}, Val Acc = {test_acc}\")\n",
    "#     logging.info(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}, Val Loss = {test_loss}, Val Acc = {test_acc}\")\n",
    "# test_preds, test_labels, test_loss, test_acc = evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"chat_summary_labels.csv\")  # Load your CSV file with data\n",
    "data_labels = data['labels'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_data_in_list(path):\n",
    "    data_entries = []\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for i in range(len(data['data'])):\n",
    "            row_entry = data['data'][i]\n",
    "            try:\n",
    "                row_specific_list = []\n",
    "                for dict in row_entry:\n",
    "                    for key in dict.keys():\n",
    "                        statement = str(key) +\" : \" + str(dict[key])\n",
    "                        row_specific_list.append(statement)\n",
    "                data_entries.append(row_specific_list)\n",
    "            except Exception as err:\n",
    "                new_row_entry = row_entry[\"conversations\"]\n",
    "                row_specific_list = []\n",
    "                for dict in new_row_entry:\n",
    "                    for key in dict.keys():\n",
    "                        statement = str(key) +\" : \" + str(dict[key])\n",
    "                        row_specific_list.append(statement)\n",
    "                data_entries.append(row_specific_list)\n",
    "    return data_entries\n",
    "data_entries = get_data_in_list(path = \"sales_conversations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAHAN SE HAI CODE :::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 289.80it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1211.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.7029, Validation Accuracy = 0.4450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 283.73it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1223.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.6957, Validation Accuracy = 0.5550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 293.42it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1121.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.6924, Validation Accuracy = 0.4870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 315.33it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1172.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.6874, Validation Accuracy = 0.5420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 300.98it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1232.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.6651, Validation Accuracy = 0.5428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 273.74it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1101.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.6317, Validation Accuracy = 0.4824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 249.91it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1200.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.5890, Validation Accuracy = 0.5054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 293.96it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1235.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.5538, Validation Accuracy = 0.4939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 316.14it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1190.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.5148, Validation Accuracy = 0.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 644/644 [00:02<00:00, 292.08it/s]\n",
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1229.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.4788, Validation Accuracy = 0.4946\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(filename='custom_transformer_training_logs.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Example Conversations and Labels (List of Lists)\n",
    "conversations = data_entries[0:550]\n",
    "labels = data_labels # Extend to match the number of conversation lists\n",
    "\n",
    "# Tokenization and Vocabulary\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "        self.idx2word = {0: \"[PAD]\", 1: \"[UNK]\"}\n",
    "        self.word_count = {}\n",
    "\n",
    "    def fit(self, texts):\n",
    "        for text_list in texts:\n",
    "            for text in text_list:  # Iterate over each conversation in the list\n",
    "                for word in text.split():\n",
    "                    if word not in self.word2idx:\n",
    "                        idx = len(self.word2idx)\n",
    "                        self.word2idx[word] = idx\n",
    "                        self.idx2word[idx] = word\n",
    "                    self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "\n",
    "    def encode(self, text, max_len):\n",
    "        tokens = [self.word2idx.get(word, 1) for word in text.split()]\n",
    "        tokens = tokens[:max_len]\n",
    "        return tokens + [0] * (max_len - len(tokens))\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Instantiate and fit tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(conversations)\n",
    "\n",
    "# Dataset Class\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        # Flatten the list of lists into a single list of conversations\n",
    "        self.texts = [item for sublist in texts for item in sublist]\n",
    "        self.labels = labels * len(texts)  # Duplicate the labels for each sublist\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode(text, self.max_len)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoding, dtype=torch.long),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Split Data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    conversations, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create Datasets and Loaders\n",
    "max_len = 50\n",
    "train_dataset = ConversationDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "test_dataset = ConversationDataset(test_texts, test_labels, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Transformer Components\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attention, value), attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.query(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        key = self.key(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.value(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        attn_output, _ = ScaledDotProductAttention()(query, key, value, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "\n",
    "# Transformer Model\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_len, num_classes, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, dropout_rate) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.sigmoid(self.fc(x))\n",
    "\n",
    "# Initialize Model\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "num_classes = 1\n",
    "model = CustomTransformer(vocab_size, d_model, n_heads, n_layers, max_len, num_classes).to(device)\n",
    "\n",
    "# Training Setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_model(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids).squeeze()\n",
    "            preds = (outputs > 0.5).float()\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, train_loader)\n",
    "    val_acc = evaluate_model(model, test_loader)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Validation Accuracy = {val_acc:.4f}\")\n",
    "    logging.info(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Validation Accuracy = {val_acc:.4f}\")\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 164/164 [00:00<00:00, 1140.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49464831804281345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = evaluate_model(model, test_loader)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab_size())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Customer Satisfaction Probability: 0.1716\n",
      "Predicted Class (Satisfaction): Not Satisfied\n"
     ]
    }
   ],
   "source": [
    "# Example conversation\n",
    "example_conversation = [\n",
    "    \"Hi, I'm looking to buy a new phone. Can you help me with that?\",\n",
    "    \"Sure! What features are you looking for?\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the example conversation\n",
    "encoded_example = tokenizer.encode(\" \".join(example_conversation), max_len)\n",
    "\n",
    "# Convert to tensor and send to the device\n",
    "input_tensor = torch.tensor([encoded_example], dtype=torch.long).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get the prediction (probability of customer satisfaction)\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor).squeeze()  # Squeeze to get a single value\n",
    "    prediction = (output > 0.5).float()  # Convert to 0 or 1 based on threshold\n",
    "    predicted_score = output.item()  # Convert the output tensor to a scalar\n",
    "\n",
    "# Print the prediction\n",
    "print(f\"Predicted Customer Satisfaction Probability: {predicted_score:.4f}\")\n",
    "print(f\"Predicted Class (Satisfaction): {'Satisfied' if prediction.item() == 1 else 'Not Satisfied'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_conversation_to_list():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_summary_list(input_text, tokenizer, model):\n",
    "    conversations\n",
    "    input_text = \" \".join(conversations)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=350,  # Adjust depending on the desired summary length\n",
    "        min_length=40,   # Minimum length of the summary\n",
    "        num_beams=4,     # Number of beams for beam search\n",
    "        length_penalty=2.0\n",
    "        \n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"kabita-choudhary/finetuned-bart-for-conversation-summary\")\n",
    "model_1 = AutoModelForSeq2SeqLM.from_pretrained(\"kabita-choudhary/finetuned-bart-for-conversation-summary\").to(device)\n",
    "print(model_1.config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
